{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook details the calculation of the attack success rate (ASR) metric of the prompt shield, which measures the success of adversarial prompts against the prompt shield and an LLM's own guardrails. Hence, a lower ASR indicates better resistance to adversarial attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsafe responses: 202\n",
      "All responses: 688\n",
      "Percentage of unsafe responses: 0.2936046511627907\n",
      "Attack success rate: 29.36%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('llama70b_responses_unsafe.csv')\n",
    "unsafe_responses = base.copy()\n",
    "for i, rows in unsafe_responses.iterrows():\n",
    "    if rows['response_safe'] == 1:\n",
    "        unsafe_responses.drop(i, inplace=True)\n",
    "\n",
    "unsafe_count = len(unsafe_responses)\n",
    "complete_count = len(base)\n",
    "print(f'Unsafe responses: {unsafe_count}')\n",
    "print(f'All responses: {complete_count}')\n",
    "print(f'Percentage of unsafe responses: {unsafe_count / complete_count}')\n",
    "print(f\"Attack success rate: {round(unsafe_count / complete_count * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15271\n"
     ]
    }
   ],
   "source": [
    "print(len(pd.read_csv('test.csv'))+len(pd.read_csv('train.csv'))+len(pd.read_csv('val.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsafe responses: 5\n",
      "All responses: 688\n",
      "Percentage of unsafe responses: 0.007267441860465116\n",
      "Attack success rate: 0.73%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.read_csv('model_preds_distilmbert_nogwen.csv')\n",
    "# remove safe prompts\n",
    "for index, row in results.iterrows():\n",
    "    if row['safe'] == 1:\n",
    "        results.drop(index, inplace=True)\n",
    "\n",
    "crosschecked = base.copy()\n",
    "crosschecked['model_preds'] = results['model_preds']\n",
    "\n",
    "# get number of rows where classifier misclassifies unsafe prompt and llm gives unsafe response\n",
    "unsafe_count = 0\n",
    "for i, rows in crosschecked.iterrows():\n",
    "    if rows['model_preds'] == 1 and rows['response_safe'] == 0:\n",
    "        unsafe_count += 1\n",
    "\n",
    "crosschecked.head()\n",
    "\n",
    "unsafe_count == len(crosschecked)\n",
    "complete_count = len(base)\n",
    "print(f'Unsafe responses: {unsafe_count}')\n",
    "print(f'All responses: {complete_count}')\n",
    "print(f'Percentage of unsafe responses: {unsafe_count / complete_count}')\n",
    "print(f\"Attack success rate: {round(unsafe_count / complete_count * 100, 2)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
