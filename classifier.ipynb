{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "\n",
    "# model_name = 'FacebookAI/xlm-roberta-base'\n",
    "model_name = 'distilbert/distilbert-base-multilingual-cased'\n",
    "train_data = pd.read_csv(\"train.csv\", encoding='latin-1')\n",
    "val_data = pd.read_csv(\"val.csv\", encoding='latin-1')\n",
    "test_data = pd.read_csv(\"test.csv\", encoding='latin-1')\n",
    "\n",
    "def tupleize(data, max_length=512):\n",
    "    data['safe'] = data['safe'].astype(int)\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    tokens = tokenizer(list(data[\"prompt\"]), return_tensors=\"np\", padding=True, truncation=True, max_length=max_length)\n",
    "    \n",
    "    token_lengths = [len(token) for token in tokens['input_ids']]\n",
    "    data_filtered = data.iloc[np.where(np.array(token_lengths) <= max_length)]\n",
    "    \n",
    "    tokens = tokenizer(list(data_filtered[\"prompt\"]), return_tensors=\"np\", padding=True, truncation=True, max_length=max_length)\n",
    "    \n",
    "    labels = np.array(data_filtered[\"safe\"])\n",
    "    \n",
    "    return (tokens, labels)\n",
    "\n",
    "train_data = tupleize(train_data)\n",
    "val_data = tupleize(val_data)\n",
    "test_data = tupleize(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(train_data[1])\n",
    "plt.hist(val_data[1])\n",
    "plt.hist(test_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import tensorflow as tf\n",
    "import datasets\n",
    "\n",
    "model = transformers.TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.compile(optimizer=transformers.AdamWeightDecay(learning_rate=3e-5), metrics=['accuracy'])  # No loss argument!\n",
    "model.fit(x=train_data[0], y=train_data[1], validation_data=(val_data[0], val_data[1]), batch_size=8, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_data[0], test_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save('model_ttv.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "model.predict(tokenizer(\"Pano magluto ng sinigang?\", return_tensors=\"np\"))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6278073,
     "sourceId": 10167475,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
